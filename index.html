<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ivaylo 'Ivo' Boyadzhiev</title>

  <meta name="author" content="Ivo Boyadzhiev">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ivaylo 'Ivo' Boyadzhiev</name>
              </p>
              <p>I am a Senior Applied Science Manager at <a href="https://www.zillow.com/z/3d-home/">Zillow Group</a>, where currently I lead a team of Applied Scientists and Machine Learning Engineers working on indoor scene reconstruction and understanding, using scalable off-the-shelf hardware solutions, like smart phones and 360 RGB cameras.
                I also co-managed the release of our open-source, free for academic research, <a href="https://github.com/zillow/zind">Zillow Indoor Dataset</a>, published at <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Cruz_Zillow_Indoor_Dataset_Annotated_Floor_Plans_With_360deg_Panoramas_and_CVPR_2021_paper.pdf">CVPR 2021</a>.
              </p>
              <p>
                At Zillow I've worked on Floor Plan Reconstruction for <a href="https://www.zillow.com/z/3d-home/floor-plans/">Interactive Floor Plans</a>, Panorama Stitching for <a href="https://www.zillow.com/z/3d-home/">3D Home&reg;</a>, Improving Video Stabilization for <a href="https://www.zillow.com/z/video-upload/">Video Walkthrough</a>, Intrinsic Image Decomposition for Zillow Digs</a>. I did my PhD at <a href="https://rgb.cs.cornell.edu/people/">Cornell University</a>, where I was advised by <a href="https://www.cs.cornell.edu/~kb/">Kavita Bala</a> and I spent 3 summers 2012-2014 working at Adobe Research under the supervision of <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>. My <a href="https://ecommons.cornell.edu/handle/1813/41097">PhD thesis</a> centered around the problems of designing algorithms to assist advanced computational photography tasks, such as lighting design, white balance under mixed lighting, and image filters for material editing.
              </p>
              <p style="text-align:center">
                <a href="mailto:ivailob@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/ivo_cv.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=RdUzcL4AAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, computational photorgraphy and image processing. Much of my recent research is about indoor reconstruction, scene understanding and wide-baseline camera localization in challenging interior settings.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/covispose.jpg" alt="covispose" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>CoVisPose: Co-Visibility Pose Transformer for Wide-Baseline Relative Pose Estimation in 360&#176; Indoor Panoramas</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/willhutchcroft">Will Hutchcroft</a>,
              <a href="https://www.researchgate.net/profile/Yuguang_Li">Yuguang Li</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://www.linkedin.com/in/ethan-wan/en">Zhiqiang Wan</a>,
              <a href="https://haiyan-chris-wang.github.io/">Haiyan Wang</a>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="">project page (coming soon)</a> /
              <!-- <a href="">supplement (coming soon)</a> / -->
              <!-- <a href="">video (coming soon)</a> / -->
              <!-- <a href="">code (coming soon)</a> / -->
              <a href="data/hutchcroft22eccv_covispose.bib">bibtex</a>
              <p> We present CoVisPose, a new end-to-end supervised learning method for relative camera pose estimation in wide baseline 360&#176; indoor panoramas.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/salve.jpg" alt="salve" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://johnwlambert.github.io/salve/">
                <papertitle>SALVe: Semantic Alignment Verification for Floorplan Reconstruction from Sparse Panoramas</papertitle>
              </a>
              <br>
              <a href="https://johnwlambert.github.io/">John W Lambert</a>,
              <a href="https://www.researchgate.net/profile/Yuguang_Li">Yuguang Li</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://www.linkedin.com/in/lambert-wixson-57b567">Lambert Wixson</a>,
              <a href="https://www.linkedin.com/in/manjunath-narayana-a790b75">Manjunath Narayana</a>,
              <a href="https://www.linkedin.com/in/willhutchcroft">Will Hutchcroft</a>,
              <a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a>,
              <a href="https://dellaert.github.io/">Frank Dellaert</a>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://johnwlambert.github.io/salve/">project page</a> /
              <a href="">supplement (coming soon)</a> /
              <a href="https://www.youtube.com/watch?v=WBOVn0LC7dI&feature=emb_imp_woyt">video</a> /
              <a href="https://github.com/zillow/salve">code</a> /
              <a href="data/lambert22eccv_salve.bib">bibtex</a>
              <p> We propose a new system for automatic 2D floorplan reconstruction that is enabled by SALVe, our novel pairwise learned alignment verifier.
                The inputs to our system are sparsely located 360&#176; panoramas, whose semantic features (windows, doors, and openings) are inferred and used
                to hypothesize pairwise room adjacency or overlap. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/zhi2022semantically.jpeg" alt="virtual_staging" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.cs.cmu.edu/~ILIM/projects/AA/ZindDecomp/">
                <papertitle>Semantically Supervised Appearance Decomposition for Virtual Staging from a Single Panorama</papertitle>
              </a>
              <br>
              <a href="https://tiancheng-zhi.github.io/">Tiancheng Zhi</a>,
              <a href="https://armastuschen.github.io/">Bowei Chen</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
              <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a>,
              <a href="http://www.cs.cmu.edu/~srinivas/">Srinivasa G. Narasimhan</a>,
              <br>
              <em>ACM ToG (SIGGRAPH)</em>, 2022
              <br>
              <a href="http://www.cs.cmu.edu/~ILIM/projects/AA/ZindDecomp/">project page</a> /
              <a href="http://www.cs.cmu.edu/~ILIM/projects/AA/ZindDecomp/files/supp.pdf">supplement</a> /
              <a href="https://github.com/tiancheng-zhi/pano_decomp">code</a> /
              <a href="data/zhi2022semantically.bib">bibtex</a>
              <p> We describe a novel, weakly-supervised, approach to decompose a single panorama of an empty indoor environment into four appearance components: specular, direct sunlight, diffuse and diffuse ambient without direct sunlight. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yin2022generating.jpg" alt="itl" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2204.12338">
                <papertitle>Generating Topological Structure of Floorplans from Room Attributes</papertitle>
              </a>
              <br>
              <a href="https://yin-yu.github.io/">Yu Yin</a>,
              <a href="https://www.linkedin.com/in/willhutchcroft">Will Hutchcroft</a>,
              <a href="https://www.najikhosravan.com/">Naji Khosravan</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
              <br>
              <em>ICMR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2204.12338.pdf">paper</a> /
              <a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531384#:~:text=30.2%20MB-,Play%20stream,-Download">video</a> /
              <a href="data/yin2022generating.bib">bibtex</a>
              <p> We propose to extract topological information from room attributes using an enhancement of GNNs that we call Iterative and adaptive graph Topology Learning (ITL). ITL progressively predicts multiple relations between rooms; at each iteration, it improves node embeddings, which in turn facilitates the generation of a better topological graph structure.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/laser_teaser.gif" alt="laser" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/zillow/zind">
                <papertitle>LASER: LAtent SpacE Rendering for 2D Visual Localization</papertitle>
              </a>
              <br>
              <a href="https://htkseason.github.io/">Zhixiang Min</a>,
              <a href="https://www.najikhosravan.com/">Naji Khosravan</a>,
              <a href="https://zachbessinger.com/">Zachary Bessinger</a>,
              <a href="https://www.linkedin.com/in/manjunath-narayana-a790b75">Manjunath Narayana</a>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
              <strong>Ivaylo Boyadzhiev</strong>
              <br>
              <em>CVPR</em>, 2022  <font COLOR="RED"><b>(Oral Presentation)</b></font>
              <br>
              <a href="https://github.com/zillow/laser">project page</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Min_LASER_LAtent_SpacE_CVPR_2022_supplemental.pdf">supplement</a> /
              <a href="https://www.youtube.com/watch?v=yb7bj2VZkSg">video</a> /
              <a href="https://github.com/zillow/laser">code</a> /
              <a href="data/min2022laser.bib">bibtex</a>
              <p> We address the problem of multimodal pose estimation, given an RGB image and a 2D schematifc floor-plan.
                We introduce the concept of latent space rendering, where 2D pose hypotheses on the floor map are directly rendered into a geometrically-structured latent space by aggregating viewing ray features.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/psmnet.jpg" alt="psmnet" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/zillow/zind">
                <papertitle>PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation</papertitle>
              </a>
              <br>
              <a href="https://haiyan-chris-wang.github.io/">Haiyan Wang</a>,
              <a href="https://www.linkedin.com/in/willhutchcroft">Will Hutchcroft</a>,
              <a href="https://www.researchgate.net/profile/Yuguang_Li">Yuguang Li</a>,
              <a href="https://www.linkedin.com/in/ethan-wan/en">Zhiqiang Wan</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/yltian.html">Yingli Tian</a>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_PSMNet_Position-Aware_Stereo_CVPR_2022_supplemental.pdf">supplement</a> /
              <a href="data/wang2022psmnet.bib">bibtex</a>
              <p>Given two-view panoramas, we propose the first network to jointly estimate the complete room layout and the refine a given coarse relative pose between them.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/zind.jpg" alt="zind" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/zillow/zind">
                <papertitle>Zillow Indoor Dataset: Annotated Floor Plans With 360&#176; Panoramas and 3D Room Layouts</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/stevecruz">Steve Cruz</a>,
              <a href="https://www.linkedin.com/in/willhutchcroft">Will Hutchcroft</a>,
              <a href="https://www.researchgate.net/profile/Yuguang_Li">Yuguang Li</a>,
              <a href="https://www.najikhosravan.com/">Naji Khosravan</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="http://www.singbingkang.com/">Sing Bing Kang</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://github.com/zillow/zind">project page</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Cruz_Zillow_Indoor_Dataset_CVPR_2021_supplemental.pdf">supplement</a> /
              <a href="data/cruz2021zillow.bib">bibtex</a>
              <p> The Zillow Indoor Dataset (ZInD) is available for download (academic research only)! It features more than 70k omnidirectional panoramas from more than 1.5k unfurnished homes, and has room layout and annotation (windows and doors) information. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/panosfm2016.jpg" alt="panosfm" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1612.01256">
                <papertitle>Panoramic Structure from Motion via Geometric Relationship Detection</papertitle>
              </a>
              <br>
              <a href="https://satoshi-ikehata.github.io/">Satoshi Ikehata</a>,
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://shanqi.github.io/">Qi Shan</a>,
              <a href="https://www.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a>
              <br>
              <em>CoRR arXiv</em>, 2016
              <br>
              <a href="https://arxiv.org/abs/1612.01256">arXiv paper</a> /
              <a href="data/ikehata2016panoramic.bib">bibtex</a>
              <p> We address challenging cases for SfM (in indoor settings) by fusing of single and multi-view reconstruction techniques via geometric relationship detection (e.g., detecting 2D lines as coplanar in 3D).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/diy16.jpg" alt="diy_light_design" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.cornell.edu/~kb/publications/diyLighting16.pdf">
                <papertitle>Do-It-Yourself Lighting Design for Product Videography</papertitle>
              </a>
              <br>
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>,
              <a href="https://www.cs.cornell.edu/~kb/">Kavita Bala</a>
              <br>
              <em>ICCP</em>, 2016
              <br>
              <a href="https://www.cs.cornell.edu/~kb/publications/diyLighting16.pdf">paper</a> /
              <a href="http://people.csail.mit.edu/sparis/publi/2016/iccp/Boydzhiev_16_DIY-Lighting-Design.mov">video</a> /
              <a href="http://people.csail.mit.edu/sparis/publi/2016/iccp/Boydzhiev_16_DIY-Lighting-Design.key">slides</a> /
              <a href="data/boyadzhiev2016yourself.bib">bibtex</a>
              <p>We propose a simple do-it-yourself (DIY) setup with a video camera on a tripod recording the product while the user waves an area light source around it for a couple of minutes. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/band_sifting.jpg" alt="band_sifting" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.cornell.edu/projects/light_compositing/">
                <papertitle>Band-Sifting Decomposition for Image Based Material Editing</papertitle>
              </a>
              <br>
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>,
              <a href="https://www.cs.cornell.edu/~kb/">Kavita Bala</a>,
              <a href="http://persci.mit.edu/people/adelson">Edward H. Adelson</a>
              <br>
              <em>ACM ToG</em>, 2015
              <br>
              <a href="https://www.cs.cornell.edu/projects/band_sifting_filters/">project page</a> /
              <a href="https://www.cs.cornell.edu/projects/band_sifting_filters/download/band_sifting_supplemental.pdf">supplement</a> /
              <a href="https://vimeo.com/139091562?embedded=true&source=video_title&owner=17987495">video</a> /
              <a href="https://www.cs.cornell.edu/projects/band_sifting_filters/download/band_sifting_all_images.zip">data</a> /
              <a href="data/boyadzhiev2013user.bib">bibtex</a>
              <p>We propose an extended family of band-sifting filters and study their perceptual effects for image-based material editing. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/light_compositing.jpg" alt="light_compositing" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.cornell.edu/projects/light_compositing/">
                <papertitle>User-assisted Image Compositing for Photographic Lighting</papertitle>
              </a>
              <br>
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>,
              <a href="https://www.cs.cornell.edu/~kb/">Kavita Bala</a>
              <br>
              <em>SIGGRAPH</em>, 2013
              <br>
              <a href="https://www.cs.cornell.edu/projects/light_compositing/">project page</a> /
              <a href="https://www.cs.cornell.edu/projects/light_compositing/download/light_compositing_supplemental.pdf">supplement</a> /
              <a href="https://vimeo.com/64976852?embedded=true&source=video_title&owner=17987495">video</a> /
              <a href="https://www.cs.cornell.edu/projects/light_compositing/download/light_compositing_input_data.zip">data</a> /
              <a href="data/boyadzhiev2013user.bib">bibtex</a>
              <p>We propose a novel user-guided lighting design method based on capturing several photos from a fixed viewpoint with a moving light source. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mixed_wb.jpg" alt="mixed_wb" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.cornell.edu/projects/white_balance/">
                <papertitle>User-guided White Balance for Mixed Lighting Conditions</papertitle>
              </a>
              <br>
              <strong>Ivaylo Boyadzhiev</strong>,
              <a href="https://www.cs.cornell.edu/~kb/">Kavita Bala</a>,
              <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>,
              <a href="http://people.csail.mit.edu/fredo/">Fr√©do Durand</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2012
              <br>
              <a href="https://www.cs.cornell.edu/projects/white_balance/">project page</a> /
              <a href="https://www.cs.cornell.edu/projects/white_balance/download/supplemental_material.pdf">supplement</a> /
              <a href="data/boyadzhiev2012user.bib">bibtex</a>
              <p>We propose a solution to the ill-posed mixed light white balance problem, based on relative reflectance properties, that are easy for humans to provide in the form of scribbles.
            </td>
          </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpeg"></td>
            <td width="75%" valign="center">
              CVPR Reviewer: 2020-2022
              <br>
              ECCV Reviewer: 2020, 2022
              <br>
              ICCV Reviewer: 2021, <a href="https://iccv2021.thecvf.com/outstanding-reviewers"> outstanding reviewer award </a>
              <br>
              SIGGRAPH Reviewer: 2015-2018, 2022
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cornell_box.jpg" alt="cs5625" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="center">
              <a href="https://www.cs.cornell.edu/courses/cs5625/2013sp/">Advanced Interactive Graphics, Spring 2013, Cornell University </a>
              <br>
              <a href="https://www.cs.cornell.edu/courses/cs4620/2011fa/about.stm">Introduction to Computer Graphics, Fall 2011, Cornell Univerity </a>
              <br>
              <a href="https://www.cs.cornell.edu/~bindel/class/cs3220-s11/syllabus.html">Scientific Computation, Spring 2011, Cornell University </a>
              <br>
              <a href="https://www.cs.cornell.edu/courses/cs4620/2010fa/about.stm">Introduction to Computer Graphics, Fall 2010, Cornell Univerity </a>
              <br>
              <a href="https://learn.fmi.uni-sofia.bg/course/view.php?id=100">Design and Analysis of Computer Algorithms, 2007 and 2008, Sofia University </a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
